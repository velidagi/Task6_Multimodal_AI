import cv2
import numpy as np
import torch
import clip
from PIL import Image
import matplotlib.pyplot as plt
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor
import requests
from transformers import BlipProcessor, BlipForConditionalGeneration
import io
from typing import List, Tuple, Dict
import json

class MultimodalBackpackExtractor:
    """
    Multimodal AI ile Ã§anta segmentasyonu
    
    KullanÄ±lan Modeller:
    1. CLIP - GÃ¶rÃ¼ntÃ¼-metin eÅŸleÅŸtirme
    2. SAM - Segment Anything Model
    3. BLIP - GÃ¶rÃ¼ntÃ¼ aÃ§Ä±klama
    """
    
    def __init__(self, sam_checkpoint_path="sam_vit_h_4b8939.pth"):
        """Multimodal modelleri yÃ¼kle"""
        print("ğŸ¤– Multimodal AI modelleri yÃ¼kleniyor...")
        
        # 1. CLIP Model (GÃ¶rÃ¼ntÃ¼-Metin EÅŸleÅŸtirme)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device=self.device)
        print("âœ… CLIP modeli yÃ¼klendi")
        
        # 2. SAM Model (Segmentasyon)
        try:
            sam = sam_model_registry["vit_h"](checkpoint=sam_checkpoint_path)
            sam.to(device=self.device)
            self.sam_predictor = SamPredictor(sam)
            self.mask_generator = SamAutomaticMaskGenerator(sam)
            print("âœ… SAM modeli yÃ¼klendi")
        except:
            print("âš ï¸ SAM modeli yÃ¼klenemedi. SAM checkpoint dosyasÄ±nÄ± indirin.")
            self.sam_predictor = None
            self.mask_generator = None
        
        # 3. BLIP Model (GÃ¶rÃ¼ntÃ¼ AÃ§Ä±klama)
        try:
            self.blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
            self.blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
            print("âœ… BLIP modeli yÃ¼klendi")
        except:
            print("âš ï¸ BLIP modeli yÃ¼klenemedi")
            self.blip_processor = None
            self.blip_model = None
        
        # Ã‡anta ile ilgili text prompt'larÄ±
        self.backpack_prompts = [
            "a backpack",
            "a bag on someone's back",
            "a rucksack",
            "a school bag",
            "a travel backpack",
            "a person wearing a backpack"
        ]
    
    def analyze_image_with_blip(self, image_path: str) -> str:
        """BLIP ile gÃ¶rÃ¼ntÃ¼yÃ¼ analiz et ve aÃ§Ä±klama Ã¼ret"""
        if self.blip_processor is None:
            return "BLIP model not available"
        
        image = Image.open(image_path).convert('RGB')
        
        # BLIP ile gÃ¶rÃ¼ntÃ¼ aÃ§Ä±klamasÄ±
        inputs = self.blip_processor(image, return_tensors="pt")
        out = self.blip_model.generate(**inputs, max_length=50)
        caption = self.blip_processor.decode(out[0], skip_special_tokens=True)
        
        return caption
    
    def find_backpack_with_clip(self, image_path: str) -> Dict:
        """CLIP kullanarak Ã§anta tespiti"""
        image = Image.open(image_path).convert('RGB')
        image_input = self.clip_preprocess(image).unsqueeze(0).to(self.device)
        
        # Text prompt'larÄ± tokenize et
        text_inputs = clip.tokenize(self.backpack_prompts).to(self.device)
        
        # Ã–zellik Ã§Ä±karma
        with torch.no_grad():
            image_features = self.clip_model.encode_image(image_input)
            text_features = self.clip_model.encode_text(text_inputs)
            
            # Benzerlik skorlarÄ±
            similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)
            values, indices = similarities[0].topk(3)
        
        # En yÃ¼ksek skorlu prompt'larÄ± dÃ¶ndÃ¼r
        results = []
        for i, (value, idx) in enumerate(zip(values, indices)):
            results.append({
                "prompt": self.backpack_prompts[idx],
                "confidence": value.item(),
                "rank": i + 1
            })
        
        return {
            "analysis": results,
            "max_confidence": results[0]["confidence"],
            "best_match": results[0]["prompt"]
        }
    
    def segment_with_sam(self, image_path: str) -> List[Dict]:
        """SAM ile otomatik segmentasyon"""
        if self.mask_generator is None:
            return []
        
        image = cv2.imread(image_path)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # SAM ile tÃ¼m segmentleri Ã¼ret
        masks = self.mask_generator.generate(image_rgb)
        
        # Maskeleri boyut ve kaliteye gÃ¶re sÄ±rala
        masks = sorted(masks, key=lambda x: x['area'], reverse=True)
        
        return masks
    
    def filter_backpack_segments(self, image_path: str, masks: List[Dict], clip_threshold: float = 0.15) -> List[Dict]:
        """CLIP skorlarÄ±na gÃ¶re Ã§anta segmentlerini filtrele"""
        image = cv2.imread(image_path)
        height, width = image.shape[:2]
        
        backpack_segments = []
        
        for i, mask_data in enumerate(masks[:20]):  # Ä°lk 20 segmenti kontrol et
            mask = mask_data['segmentation']
            bbox = mask_data['bbox']
            
            # Segment Ã§ok kÃ¼Ã§Ã¼kse atla
            if mask_data['area'] < (height * width * 0.01):  # %1'den kÃ¼Ã§Ã¼k
                continue
            
            # MaskelenmiÅŸ bÃ¶lgeyi Ã§Ä±kar
            x, y, w, h = map(int, bbox)
            
            # Bounding box sÄ±nÄ±rlarÄ±nÄ± kontrol et
            x = max(0, x)
            y = max(0, y)
            w = min(w, width - x)
            h = min(h, height - y)
            
            if w <= 0 or h <= 0:
                continue
            
            # Segment bÃ¶lgesini kÄ±rp
            segment_region = image[y:y+h, x:x+w]
            
            if segment_region.size == 0:
                continue
            
            # GeÃ§ici dosya olarak kaydet
            temp_path = f"temp_segment_{i}.jpg"
            cv2.imwrite(temp_path, segment_region)
            
            try:
                # CLIP ile bu segmentin Ã§anta olup olmadÄ±ÄŸÄ±nÄ± kontrol et
                clip_result = self.find_backpack_with_clip(temp_path)
                
                if clip_result["max_confidence"] > clip_threshold:
                    backpack_segments.append({
                        "mask": mask,
                        "bbox": bbox,
                        "area": mask_data['area'],
                        "clip_confidence": clip_result["max_confidence"],
                        "clip_match": clip_result["best_match"],
                        "segment_id": i
                    })
                
                # GeÃ§ici dosyayÄ± sil
                import os
                if os.path.exists(temp_path):
                    os.remove(temp_path)
                    
            except Exception as e:
                print(f"Segment {i} iÅŸlenirken hata: {e}")
                continue
        
        # CLIP skoruna gÃ¶re sÄ±rala
        backpack_segments.sort(key=lambda x: x['clip_confidence'], reverse=True)
        
        return backpack_segments
    
    def create_final_mask(self, image_path: str, backpack_segments: List[Dict]) -> np.ndarray:
        """En iyi Ã§anta segmentlerinden final mask oluÅŸtur"""
        image = cv2.imread(image_path)
        height, width = image.shape[:2]
        
        final_mask = np.zeros((height, width), dtype=np.uint8)
        
        # En yÃ¼ksek skorlu segmentleri birleÅŸtir
        for segment in backpack_segments[:2]:  # En iyi 2 segmenti al
            mask = segment['mask'].astype(np.uint8) * 255
            final_mask = cv2.bitwise_or(final_mask, mask)
        
        return final_mask
    
    def process_multimodal(self, image_path: str, output_path: str = "multimodal_canta.png") -> Dict:
        """Multimodal AI pipeline ile Ã§anta Ã§Ä±karma"""
        print(f"ğŸ¯ Multimodal AI ile analiz baÅŸlÄ±yor: {image_path}")
        
        results = {
            "image_path": image_path,
            "output_path": output_path,
            "steps": {}
        }
        
        # 1. BLIP ile gÃ¶rÃ¼ntÃ¼ aÃ§Ä±klamasÄ±
        print("ğŸ“ BLIP ile gÃ¶rÃ¼ntÃ¼ aÃ§Ä±klanÄ±yor...")
        blip_caption = self.analyze_image_with_blip(image_path)
        results["steps"]["blip_caption"] = blip_caption
        print(f"BLIP AÃ§Ä±klamasÄ±: {blip_caption}")
        
        # 2. CLIP ile Ã§anta varlÄ±ÄŸÄ± kontrolÃ¼
        print("ğŸ” CLIP ile Ã§anta aranÄ±yor...")
        clip_result = self.find_backpack_with_clip(image_path)
        results["steps"]["clip_analysis"] = clip_result
        print(f"CLIP En Ä°yi EÅŸleÅŸme: {clip_result['best_match']} ({clip_result['max_confidence']:.3f})")
        
        # 3. SAM ile segmentasyon
        print("âœ‚ï¸ SAM ile segmentasyon yapÄ±lÄ±yor...")
        all_masks = self.segment_with_sam(image_path)
        results["steps"]["total_segments"] = len(all_masks)
        print(f"Toplam {len(all_masks)} segment bulundu")
        
        # 4. CLIP ile segment filtreleme
        print("ğŸ¯ Ã‡anta segmentleri filtreleniyor...")
        backpack_segments = self.filter_backpack_segments(image_path, all_masks)
        results["steps"]["backpack_segments"] = len(backpack_segments)
        results["steps"]["best_segments"] = [
            {
                "clip_confidence": seg["clip_confidence"],
                "clip_match": seg["clip_match"],
                "area": seg["area"]
            }
            for seg in backpack_segments[:3]
        ]
        
        if not backpack_segments:
            print("âŒ Ã‡anta segmenti bulunamadÄ±!")
            results["success"] = False
            return results
        
        print(f"âœ… {len(backpack_segments)} Ã§anta segmenti bulundu")
        
        # 5. Final mask oluÅŸturma
        print("ğŸ¨ Final mask oluÅŸturuluyor...")
        final_mask = self.create_final_mask(image_path, backpack_segments)
        
        # 6. Ã‡antayÄ± Ã§Ä±karma
        image = cv2.imread(image_path)
        
        # Mask uygula
        result_image = cv2.bitwise_and(image, image, mask=final_mask)
        
        # Alpha channel ekle (ÅŸeffaf arka plan)
        result_rgba = cv2.cvtColor(result_image, cv2.COLOR_BGR2BGRA)
        result_rgba[:, :, 3] = final_mask  # Alpha channel = mask
        
        # Kaydet
        cv2.imwrite(output_path, result_rgba)
        
        results["success"] = True
        results["final_confidence"] = backpack_segments[0]["clip_confidence"]
        
        print(f"ğŸ’¾ SonuÃ§ kaydedildi: {output_path}")
        
        return results
    
    def visualize_analysis(self, image_path: str, results: Dict, save_path: str = "analysis_visualization.png"):
        """Analiz sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtir"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # Orijinal gÃ¶rÃ¼ntÃ¼
        original = cv2.imread(image_path)
        original_rgb = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)
        axes[0, 0].imshow(original_rgb)
        axes[0, 0].set_title("Orijinal GÃ¶rÃ¼ntÃ¼")
        axes[0, 0].axis('off')
        
        # CLIP analizi
        clip_data = results["steps"]["clip_analysis"]["analysis"]
        prompts = [item["prompt"] for item in clip_data]
        confidences = [item["confidence"] for item in clip_data]
        
        axes[0, 1].bar(range(len(prompts)), confidences)
        axes[0, 1].set_title("CLIP GÃ¼ven SkorlarÄ±")
        axes[0, 1].set_xticks(range(len(prompts)))
        axes[0, 1].set_xticklabels(prompts, rotation=45, ha='right')
        axes[0, 1].set_ylabel("GÃ¼ven Skoru")
        
        # SonuÃ§ gÃ¶rÃ¼ntÃ¼sÃ¼
        if results["success"]:
            result_img = cv2.imread(results["output_path"], cv2.IMREAD_UNCHANGED)
            if result_img.shape[2] == 4:  # RGBA
                result_rgb = cv2.cvtColor(result_img, cv2.COLOR_BGRA2RGB)
            else:
                result_rgb = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)
            axes[1, 0].imshow(result_rgb)
            axes[1, 0].set_title("Ã‡Ä±karÄ±lan Ã‡anta")
        else:
            axes[1, 0].text(0.5, 0.5, "Ã‡anta BulunamadÄ±", ha='center', va='center')
            axes[1, 0].set_title("SonuÃ§")
        axes[1, 0].axis('off')
        
        # Ä°statistikler
        stats_text = f"""
        BLIP AÃ§Ä±klamasÄ±: {results['steps']['blip_caption']}
        
        Toplam Segment: {results['steps']['total_segments']}
        Ã‡anta Segmenti: {results['steps']['backpack_segments']}
        
        En Ä°yi EÅŸleÅŸme: {results['steps']['clip_analysis']['best_match']}
        GÃ¼ven Skoru: {results['steps']['clip_analysis']['max_confidence']:.3f}
        """
        
        axes[1, 1].text(0.1, 0.5, stats_text, fontsize=10, verticalalignment='center')
        axes[1, 1].set_title("Analiz Ä°statistikleri")
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š Analiz gÃ¶rseli kaydedildi: {save_path}")

def download_sam_checkpoint():
    """SAM model checkpoint'ini indir"""
    url = "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
    filename = "sam_vit_h_4b8939.pth"
    
    print("ğŸ“¥ SAM checkpoint indiriliyor... (Bu biraz zaman alabilir)")
    
    response = requests.get(url, stream=True)
    total_size = int(response.headers.get('content-length', 0))
    
    with open(filename, 'wb') as file:
        downloaded = 0
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                file.write(chunk)
                downloaded += len(chunk)
                if total_size > 0:
                    percent = (downloaded / total_size) * 100
                    print(f"\rÄ°ndiriliyor: {percent:.1f}%", end="")
    
    print(f"\nâœ… SAM checkpoint indirildi: {filename}")
    return filename

# Ana kullanÄ±m fonksiyonu
def extract_backpack_multimodal(image_path: str, output_path: str = "multimodal_canta.png"):
    """Multimodal AI ile Ã§anta Ã§Ä±karma - Basit kullanÄ±m"""
    
    # SAM checkpoint kontrolÃ¼
    checkpoint_path = "sam_vit_h_4b8939.pth"
    import os
    if not os.path.exists(checkpoint_path):
        print("SAM checkpoint bulunamadÄ±, indiriliyor...")
        checkpoint_path = download_sam_checkpoint()
    
    # Multimodal extractor oluÅŸtur
    extractor = MultimodalBackpackExtractor(checkpoint_path)
    
    # Ä°ÅŸlemi Ã§alÄ±ÅŸtÄ±r
    results = extractor.process_multimodal(image_path, output_path)
    
    # SonuÃ§larÄ± gÃ¶rselleÅŸtir
    extractor.visualize_analysis(image_path, results)
    
    return results

# Akademik DeÄŸerlendirme Raporu
def generate_academic_report(results: Dict) -> str:
    """Akademik rapor oluÅŸtur"""
    report = f"""
# Multimodal AI ile Ã‡anta Segmentasyonu - Akademik Rapor

## 1. KullanÄ±lan Modeller

### 1.1 CLIP (Contrastive Language-Image Pre-training)
- **AmaÃ§**: GÃ¶rÃ¼ntÃ¼-metin eÅŸleÅŸtirme ile Ã§anta tespiti
- **GÃ¼ven Skoru**: {results['steps']['clip_analysis']['max_confidence']:.3f}
- **En Ä°yi EÅŸleÅŸme**: {results['steps']['clip_analysis']['best_match']}

### 1.2 BLIP (Bootstrapping Language-Image Pre-training)
- **AmaÃ§**: GÃ¶rÃ¼ntÃ¼ aÃ§Ä±klamasÄ± ve sahne anlama
- **AÃ§Ä±klama**: {results['steps']['blip_caption']}

### 1.3 SAM (Segment Anything Model)
- **AmaÃ§**: Universal segmentasyon
- **Toplam Segment**: {results['steps']['total_segments']}
- **Ã‡anta Segmenti**: {results['steps']['backpack_segments']}

## 2. Multimodal Pipeline

1. **Vision-Language Understanding**: CLIP ve BLIP ile gÃ¶rÃ¼ntÃ¼ analizi
2. **Zero-shot Detection**: Ã–nceden eÄŸitilmemiÅŸ nesne tespiti
3. **Semantic Segmentation**: SAM ile piksel seviyesi ayÄ±rma
4. **Cross-modal Filtering**: Text prompt'lar ile segment filtreleme

## 3. SonuÃ§lar

- **BaÅŸarÄ± Durumu**: {'BaÅŸarÄ±lÄ±' if results['success'] else 'BaÅŸarÄ±sÄ±z'}
- **Final GÃ¼ven Skoru**: {results.get('final_confidence', 0):.3f}
- **Ã‡Ä±ktÄ± DosyasÄ±**: {results['output_path']}

## 4. Akademik KatkÄ±

Bu proje multimodal AI'Ä±n pratik uygulamasÄ±nÄ± gÃ¶stermektedir:
- Vision-Language modellerinin entegrasyonu
- Zero-shot learning yaklaÅŸÄ±mÄ±
- Cross-modal attention mekanizmasÄ±
"""
    
    return report

# Ana kullanÄ±m
if __name__ == "__main__":
    print("ğŸ“ Multimodal AI ile Ã‡anta Segmentasyonu - Akademik Proje")
    print("=" * 60)
    
    # GÃ¶rÃ¼ntÃ¼ yolu
    image_path = "data/canta.jpg"  # Kendi fotoÄŸrafÄ±nÄ±zÄ±n yolu
    
    try:
        # Multimodal analiz
        results = extract_backpack_multimodal(image_path)
        
        # Akademik rapor
        report = generate_academic_report(results)
        
        # Raporu kaydet
        with open("academic_report.md", "w", encoding="utf-8") as f:
            f.write(report)
        
        print("\nğŸ“‹ Akademik rapor 'academic_report.md' dosyasÄ±na kaydedildi")
        
    except Exception as e:
        print(f"âŒ Hata oluÅŸtu: {str(e)}")
        print("\nğŸ’¡ Gerekli kÃ¼tÃ¼phaneleri yÃ¼klediÄŸinizden emin olun:")
        print("pip install torch torchvision clip-by-openai segment-anything transformers matplotlib")